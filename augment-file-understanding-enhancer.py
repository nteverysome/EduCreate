#!/usr/bin/env python3
"""
Augment æª”æ¡ˆç†è§£å¢å¼·å™¨
æå‡ Augment Context Engine å°æª”æ¡ˆçš„ç†è§£å’ŒæŒæ¡åº¦
"""

import os
import json
import ast
import re
from typing import Dict, List, Any, Optional
from pathlib import Path
import hashlib
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class FileAnalysis:
    """æª”æ¡ˆåˆ†æçµæœ"""
    path: str
    type: str
    size: int
    last_modified: str
    complexity_score: int
    dependencies: List[str]
    exports: List[str]
    imports: List[str]
    functions: List[str]
    classes: List[str]
    components: List[str]
    apis: List[str]
    tests: List[str]
    documentation: str
    business_logic: List[str]
    performance_notes: List[str]
    security_notes: List[str]
    accessibility_notes: List[str]
    memory_science_notes: List[str]

class AugmentFileUnderstandingEnhancer:
    """Augment æª”æ¡ˆç†è§£å¢å¼·å™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.analysis_cache = {}
        self.project_knowledge = {}
        self.load_project_knowledge()
    
    def load_project_knowledge(self):
        """è¼‰å…¥é …ç›®çŸ¥è­˜åº«"""
        knowledge_file = self.project_root / "augment-project-knowledge.json"
        if knowledge_file.exists():
            with open(knowledge_file, 'r', encoding='utf-8') as f:
                self.project_knowledge = json.load(f)
        else:
            self.project_knowledge = {
                "project_type": "memory-science-education-platform",
                "tech_stack": {
                    "frontend": "Next.js + React + TypeScript + Tailwind",
                    "backend": "Node.js + Express + PostgreSQL + Prisma",
                    "ai": "OpenAI GPT-4 + èªéŸ³è­˜åˆ¥/åˆæˆ",
                    "testing": "Jest + Playwright"
                },
                "key_concepts": [
                    "è¨˜æ†¶ç§‘å­¸", "é–“éš”é‡è¤‡", "ä¸»å‹•å›æ†¶", "èªçŸ¥è² è·",
                    "GEPTåˆ†ç´š", "ç„¡éšœç¤™è¨­è¨ˆ", "AIå°è©±", "éŠæˆ²å¼•æ“"
                ],
                "architecture_patterns": [
                    "é˜²æ­¢åŠŸèƒ½å­¤ç«‹å·¥ä½œæµç¨‹",
                    "äº”é …åŒæ­¥é–‹ç™¼",
                    "ä¸‰å±¤æ•´åˆé©—è­‰",
                    "Playwrightç«¯åˆ°ç«¯æ¸¬è©¦"
                ]
            }
    
    def analyze_file(self, file_path: str) -> FileAnalysis:
        """æ·±åº¦åˆ†æå–®å€‹æª”æ¡ˆ"""
        path = Path(file_path)
        
        if not path.exists():
            raise FileNotFoundError(f"æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        
        # æª¢æŸ¥ç·©å­˜
        file_hash = self.get_file_hash(path)
        if file_hash in self.analysis_cache:
            return self.analysis_cache[file_hash]
        
        # åŸºæœ¬æª”æ¡ˆä¿¡æ¯
        stat = path.stat()
        file_type = self.determine_file_type(path)
        
        analysis = FileAnalysis(
            path=str(path),
            type=file_type,
            size=stat.st_size,
            last_modified=datetime.fromtimestamp(stat.st_mtime).isoformat(),
            complexity_score=0,
            dependencies=[],
            exports=[],
            imports=[],
            functions=[],
            classes=[],
            components=[],
            apis=[],
            tests=[],
            documentation="",
            business_logic=[],
            performance_notes=[],
            security_notes=[],
            accessibility_notes=[],
            memory_science_notes=[]
        )
        
        # è®€å–æª”æ¡ˆå…§å®¹
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            # äºŒé€²åˆ¶æª”æ¡ˆ
            analysis.documentation = "äºŒé€²åˆ¶æª”æ¡ˆï¼Œç„¡æ³•åˆ†æå…§å®¹"
            return analysis
        
        # æ ¹æ“šæª”æ¡ˆé¡å‹é€²è¡Œå°ˆé–€åˆ†æ
        if file_type == "typescript" or file_type == "javascript":
            self.analyze_typescript_javascript(content, analysis)
        elif file_type == "react":
            self.analyze_react_component(content, analysis)
        elif file_type == "api":
            self.analyze_api_route(content, analysis)
        elif file_type == "test":
            self.analyze_test_file(content, analysis)
        elif file_type == "markdown":
            self.analyze_markdown(content, analysis)
        elif file_type == "json":
            self.analyze_json_config(content, analysis)
        
        # EduCreate å°ˆç”¨åˆ†æ
        self.analyze_educreat_specific(content, analysis)
        
        # è¨ˆç®—è¤‡é›œåº¦åˆ†æ•¸
        analysis.complexity_score = self.calculate_complexity_score(analysis)
        
        # ç·©å­˜çµæœ
        self.analysis_cache[file_hash] = analysis
        
        return analysis
    
    def determine_file_type(self, path: Path) -> str:
        """åˆ¤æ–·æª”æ¡ˆé¡å‹"""
        suffix = path.suffix.lower()
        name = path.name.lower()
        
        if suffix in ['.ts', '.tsx']:
            if 'test' in name or 'spec' in name:
                return "test"
            elif suffix == '.tsx' or 'component' in name:
                return "react"
            elif 'api' in str(path) or 'route' in name:
                return "api"
            else:
                return "typescript"
        elif suffix in ['.js', '.jsx']:
            return "javascript"
        elif suffix == '.md':
            return "markdown"
        elif suffix == '.json':
            return "json"
        elif suffix in ['.css', '.scss']:
            return "stylesheet"
        else:
            return "other"
    
    def analyze_typescript_javascript(self, content: str, analysis: FileAnalysis):
        """åˆ†æ TypeScript/JavaScript æª”æ¡ˆ"""
        
        # æå– imports
        import_pattern = r'import\s+.*?\s+from\s+[\'"]([^\'"]+)[\'"]'
        analysis.imports = re.findall(import_pattern, content)
        
        # æå– exports
        export_pattern = r'export\s+(?:default\s+)?(?:class|function|const|let|var)\s+(\w+)'
        analysis.exports = re.findall(export_pattern, content)
        
        # æå–å‡½æ•¸
        function_pattern = r'(?:function\s+(\w+)|const\s+(\w+)\s*=\s*(?:async\s+)?\()'
        matches = re.findall(function_pattern, content)
        analysis.functions = [match[0] or match[1] for match in matches if match[0] or match[1]]
        
        # æå–é¡åˆ¥
        class_pattern = r'class\s+(\w+)'
        analysis.classes = re.findall(class_pattern, content)
        
        # æå– API èª¿ç”¨
        api_pattern = r'(?:fetch|axios|api)\s*\(\s*[\'"]([^\'"]+)[\'"]'
        analysis.apis = re.findall(api_pattern, content)
    
    def analyze_react_component(self, content: str, analysis: FileAnalysis):
        """åˆ†æ React çµ„ä»¶"""
        self.analyze_typescript_javascript(content, analysis)
        
        # æå–çµ„ä»¶åç¨±
        component_pattern = r'(?:export\s+default\s+)?(?:function|const)\s+(\w+)'
        analysis.components = re.findall(component_pattern, content)
        
        # æª¢æŸ¥ç„¡éšœç¤™è¨­è¨ˆ
        if 'aria-' in content or 'role=' in content or 'data-testid' in content:
            analysis.accessibility_notes.append("åŒ…å«ç„¡éšœç¤™è¨­è¨ˆå±¬æ€§")
        
        # æª¢æŸ¥æ¸¬è©¦ ID
        testid_pattern = r'data-testid=[\'"]([^\'"]+)[\'"]'
        test_ids = re.findall(testid_pattern, content)
        if test_ids:
            analysis.tests.extend(test_ids)
    
    def analyze_api_route(self, content: str, analysis: FileAnalysis):
        """åˆ†æ API è·¯ç”±"""
        self.analyze_typescript_javascript(content, analysis)
        
        # æå– HTTP æ–¹æ³•
        http_methods = ['GET', 'POST', 'PUT', 'DELETE', 'PATCH']
        for method in http_methods:
            if f'export async function {method}' in content:
                analysis.apis.append(f"{method} endpoint")
        
        # æª¢æŸ¥èº«ä»½é©—è­‰
        if 'getServerSession' in content or 'auth' in content:
            analysis.security_notes.append("åŒ…å«èº«ä»½é©—è­‰")
        
        # æª¢æŸ¥æ•¸æ“šé©—è­‰
        if 'zod' in content or 'joi' in content or 'yup' in content:
            analysis.security_notes.append("åŒ…å«æ•¸æ“šé©—è­‰")
    
    def analyze_test_file(self, content: str, analysis: FileAnalysis):
        """åˆ†ææ¸¬è©¦æª”æ¡ˆ"""
        
        # æå–æ¸¬è©¦æè¿°
        test_pattern = r'(?:test|it)\s*\(\s*[\'"]([^\'"]+)[\'"]'
        analysis.tests = re.findall(test_pattern, content)
        
        # æª¢æŸ¥æ¸¬è©¦é¡å‹
        if 'playwright' in content.lower():
            analysis.business_logic.append("Playwright ç«¯åˆ°ç«¯æ¸¬è©¦")
        if 'jest' in content.lower():
            analysis.business_logic.append("Jest å–®å…ƒæ¸¬è©¦")
        
        # æª¢æŸ¥é˜²æ­¢åŠŸèƒ½å­¤ç«‹æ¸¬è©¦
        if 'é˜²æ­¢åŠŸèƒ½å­¤ç«‹' in content or 'anti-isolation' in content:
            analysis.business_logic.append("é˜²æ­¢åŠŸèƒ½å­¤ç«‹é©—è­‰")
    
    def analyze_markdown(self, content: str, analysis: FileAnalysis):
        """åˆ†æ Markdown æ–‡æª”"""
        
        # æå–æ¨™é¡Œ
        title_pattern = r'^#+\s+(.+)$'
        titles = re.findall(title_pattern, content, re.MULTILINE)
        analysis.documentation = f"åŒ…å« {len(titles)} å€‹æ¨™é¡Œ: {', '.join(titles[:5])}"
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¶æ§‹æ–‡æª”
        if any(keyword in content.lower() for keyword in ['æ¶æ§‹', 'architecture', 'è¨­è¨ˆ', 'design']):
            analysis.business_logic.append("æ¶æ§‹è¨­è¨ˆæ–‡æª”")
    
    def analyze_json_config(self, content: str, analysis: FileAnalysis):
        """åˆ†æ JSON é…ç½®æª”æ¡ˆ"""
        try:
            data = json.loads(content)
            analysis.documentation = f"JSON é…ç½®æª”æ¡ˆï¼ŒåŒ…å« {len(data)} å€‹é ‚ç´šéµ"
            
            # æª¢æŸ¥ç‰¹å®šé…ç½®é¡å‹
            if 'scripts' in data:
                analysis.business_logic.append("npm è…³æœ¬é…ç½®")
            if 'dependencies' in data:
                analysis.dependencies = list(data.get('dependencies', {}).keys())
        except json.JSONDecodeError:
            analysis.documentation = "ç„¡æ•ˆçš„ JSON æ ¼å¼"
    
    def analyze_educreat_specific(self, content: str, analysis: FileAnalysis):
        """EduCreate å°ˆç”¨åˆ†æ"""
        
        # è¨˜æ†¶ç§‘å­¸ç›¸é—œ
        memory_keywords = [
            'é–“éš”é‡è¤‡', 'spaced repetition', 'ä¸»å‹•å›æ†¶', 'active recall',
            'èªçŸ¥è² è·', 'cognitive load', 'è¨˜æ†¶éŠæˆ²', 'memory game'
        ]
        
        for keyword in memory_keywords:
            if keyword.lower() in content.lower():
                analysis.memory_science_notes.append(f"åŒ…å«è¨˜æ†¶ç§‘å­¸æ¦‚å¿µ: {keyword}")
        
        # GEPT åˆ†ç´šç›¸é—œ
        if 'gept' in content.lower() or 'GEPTLevel' in content:
            analysis.business_logic.append("GEPT åˆ†ç´šç³»çµ±ç›¸é—œ")
        
        # AI å°è©±ç›¸é—œ
        if 'ai' in content.lower() and ('dialogue' in content.lower() or 'å°è©±' in content):
            analysis.business_logic.append("AI å°è©±ç³»çµ±ç›¸é—œ")
        
        # ç„¡éšœç¤™è¨­è¨ˆ
        accessibility_keywords = ['wcag', 'aria', 'accessibility', 'ç„¡éšœç¤™']
        for keyword in accessibility_keywords:
            if keyword.lower() in content.lower():
                analysis.accessibility_notes.append(f"ç„¡éšœç¤™è¨­è¨ˆ: {keyword}")
    
    def calculate_complexity_score(self, analysis: FileAnalysis) -> int:
        """è¨ˆç®—æª”æ¡ˆè¤‡é›œåº¦åˆ†æ•¸ (1-10)"""
        score = 1
        
        # åŸºæ–¼æª”æ¡ˆå¤§å°
        if analysis.size > 10000:
            score += 2
        elif analysis.size > 5000:
            score += 1
        
        # åŸºæ–¼ä¾è³´æ•¸é‡
        score += min(len(analysis.dependencies), 3)
        
        # åŸºæ–¼å‡½æ•¸å’Œé¡åˆ¥æ•¸é‡
        score += min(len(analysis.functions) + len(analysis.classes), 3)
        
        # åŸºæ–¼æ¥­å‹™é‚è¼¯è¤‡é›œåº¦
        score += min(len(analysis.business_logic), 2)
        
        return min(score, 10)
    
    def get_file_hash(self, path: Path) -> str:
        """ç²å–æª”æ¡ˆå“ˆå¸Œå€¼ç”¨æ–¼ç·©å­˜"""
        stat = path.stat()
        content = f"{path}:{stat.st_size}:{stat.st_mtime}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´å€‹é …ç›®"""
        
        print("ğŸ” é–‹å§‹åˆ†æ EduCreate é …ç›®...")
        
        # è¦åˆ†æçš„æª”æ¡ˆæ¨¡å¼
        patterns = [
            "**/*.ts", "**/*.tsx", "**/*.js", "**/*.jsx",
            "**/*.md", "**/*.json"
        ]
        
        all_files = []
        for pattern in patterns:
            all_files.extend(self.project_root.glob(pattern))
        
        # éæ¿¾æ‰ä¸éœ€è¦çš„æª”æ¡ˆ
        exclude_patterns = [
            "node_modules", ".next", "dist", "build", 
            ".git", "coverage", "test-results"
        ]
        
        filtered_files = []
        for file_path in all_files:
            if not any(exclude in str(file_path) for exclude in exclude_patterns):
                filtered_files.append(file_path)
        
        print(f"ğŸ“ æ‰¾åˆ° {len(filtered_files)} å€‹æª”æ¡ˆéœ€è¦åˆ†æ")
        
        # åˆ†ææ¯å€‹æª”æ¡ˆ
        analyses = []
        for i, file_path in enumerate(filtered_files):
            try:
                analysis = self.analyze_file(str(file_path))
                analyses.append(analysis)
                
                if (i + 1) % 50 == 0:
                    print(f"   å·²åˆ†æ {i + 1}/{len(filtered_files)} å€‹æª”æ¡ˆ")
                    
            except Exception as e:
                print(f"   âš ï¸ åˆ†ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
        
        # ç”Ÿæˆé …ç›®ç¸½çµ
        project_summary = self.generate_project_summary(analyses)
        
        # ä¿å­˜åˆ†æçµæœ
        self.save_analysis_results(analyses, project_summary)
        
        print("âœ… é …ç›®åˆ†æå®Œæˆï¼")
        return project_summary
    
    def generate_project_summary(self, analyses: List[FileAnalysis]) -> Dict[str, Any]:
        """ç”Ÿæˆé …ç›®ç¸½çµ"""
        
        summary = {
            "total_files": len(analyses),
            "file_types": {},
            "complexity_distribution": {},
            "key_components": [],
            "api_endpoints": [],
            "test_coverage": [],
            "memory_science_features": [],
            "accessibility_features": [],
            "dependencies": set(),
            "recommendations": []
        }
        
        for analysis in analyses:
            # æª”æ¡ˆé¡å‹çµ±è¨ˆ
            file_type = analysis.type
            summary["file_types"][file_type] = summary["file_types"].get(file_type, 0) + 1
            
            # è¤‡é›œåº¦åˆ†å¸ƒ
            complexity = analysis.complexity_score
            summary["complexity_distribution"][complexity] = summary["complexity_distribution"].get(complexity, 0) + 1
            
            # æ”¶é›†é—œéµä¿¡æ¯
            summary["key_components"].extend(analysis.components)
            summary["api_endpoints"].extend(analysis.apis)
            summary["test_coverage"].extend(analysis.tests)
            summary["memory_science_features"].extend(analysis.memory_science_notes)
            summary["accessibility_features"].extend(analysis.accessibility_notes)
            summary["dependencies"].update(analysis.dependencies)
        
        # è½‰æ› set ç‚º list
        summary["dependencies"] = list(summary["dependencies"])
        
        # ç”Ÿæˆå»ºè­°
        summary["recommendations"] = self.generate_recommendations(summary)
        
        return summary
    
    def generate_recommendations(self, summary: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        # è¤‡é›œåº¦å»ºè­°
        high_complexity_files = summary["complexity_distribution"].get(9, 0) + summary["complexity_distribution"].get(10, 0)
        if high_complexity_files > 0:
            recommendations.append(f"ç™¼ç¾ {high_complexity_files} å€‹é«˜è¤‡é›œåº¦æª”æ¡ˆï¼Œå»ºè­°é‡æ§‹")
        
        # æ¸¬è©¦è¦†è“‹ç‡å»ºè­°
        test_files = summary["file_types"].get("test", 0)
        total_files = summary["total_files"]
        if test_files / total_files < 0.3:
            recommendations.append("æ¸¬è©¦è¦†è“‹ç‡åä½ï¼Œå»ºè­°å¢åŠ æ›´å¤šæ¸¬è©¦")
        
        # è¨˜æ†¶ç§‘å­¸åŠŸèƒ½å»ºè­°
        if len(summary["memory_science_features"]) < 10:
            recommendations.append("è¨˜æ†¶ç§‘å­¸åŠŸèƒ½å¯¦ç¾è¼ƒå°‘ï¼Œå»ºè­°åŠ å¼·ç›¸é—œåŠŸèƒ½")
        
        # ç„¡éšœç¤™è¨­è¨ˆå»ºè­°
        if len(summary["accessibility_features"]) < 5:
            recommendations.append("ç„¡éšœç¤™è¨­è¨ˆåŠŸèƒ½è¼ƒå°‘ï¼Œå»ºè­°åŠ å¼· WCAG åˆè¦æ€§")
        
        return recommendations
    
    def save_analysis_results(self, analyses: List[FileAnalysis], summary: Dict[str, Any]):
        """ä¿å­˜åˆ†æçµæœ"""
        
        # ä¿å­˜è©³ç´°åˆ†æçµæœ
        results = {
            "timestamp": datetime.now().isoformat(),
            "project_summary": summary,
            "file_analyses": [asdict(analysis) for analysis in analyses]
        }
        
        output_file = self.project_root / "augment-file-analysis-results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š åˆ†æçµæœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•¸"""
    project_root = "C:/Users/Administrator/Desktop/EduCreate"
    
    enhancer = AugmentFileUnderstandingEnhancer(project_root)
    summary = enhancer.analyze_project()
    
    print("\nğŸ“Š é …ç›®åˆ†æç¸½çµ:")
    print(f"   ç¸½æª”æ¡ˆæ•¸: {summary['total_files']}")
    print(f"   æª”æ¡ˆé¡å‹: {summary['file_types']}")
    print(f"   é—œéµçµ„ä»¶: {len(summary['key_components'])} å€‹")
    print(f"   API ç«¯é»: {len(summary['api_endpoints'])} å€‹")
    print(f"   æ¸¬è©¦è¦†è“‹: {len(summary['test_coverage'])} å€‹æ¸¬è©¦")
    print(f"   è¨˜æ†¶ç§‘å­¸åŠŸèƒ½: {len(summary['memory_science_features'])} å€‹")
    print(f"   ç„¡éšœç¤™åŠŸèƒ½: {len(summary['accessibility_features'])} å€‹")
    
    if summary['recommendations']:
        print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
        for rec in summary['recommendations']:
            print(f"   â€¢ {rec}")

if __name__ == "__main__":
    main()
