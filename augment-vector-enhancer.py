#!/usr/bin/env python3
"""
Augment å‘é‡æ•¸æ“šåº«å¢å¼·å™¨
æä¾›èªç¾©ä»£ç¢¼æœç´¢å’Œæ™ºèƒ½ç†è§£èƒ½åŠ›
"""

import os
import json
import hashlib
import sqlite3
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
# import numpy as np  # ä¸ä½¿ç”¨ numpyï¼Œä½¿ç”¨ç´” Python å¯¦ç¾
from datetime import datetime
import logging
import re

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SimpleVectorDatabase:
    """ç°¡åŒ–ç‰ˆå‘é‡æ•¸æ“šåº« (ä¸ä¾è³´å¤–éƒ¨åº«)"""
    
    def __init__(self, db_path: str = "augment_vectors.db"):
        self.db_path = db_path
        self.init_database()
        self.vector_cache = {}  # è¨˜æ†¶é«”ç·©å­˜
        
    def init_database(self):
        """åˆå§‹åŒ–å‘é‡æ•¸æ“šåº«"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ä»£ç¢¼å‘é‡è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS code_vectors (
                id TEXT PRIMARY KEY,
                file_path TEXT NOT NULL,
                content_hash TEXT NOT NULL,
                content_text TEXT NOT NULL,
                vector_data TEXT NOT NULL,
                metadata TEXT,
                created_at TEXT,
                updated_at TEXT
            )
        ''')
        
        # èªç¾©æœç´¢ç´¢å¼•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS semantic_index (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                term TEXT NOT NULL,
                file_path TEXT NOT NULL,
                relevance_score REAL DEFAULT 1.0,
                context TEXT,
                created_at TEXT
            )
        ''')
        
        # ä»£ç¢¼ç›¸ä¼¼æ€§è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS code_similarity (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_file TEXT NOT NULL,
                target_file TEXT NOT NULL,
                similarity_score REAL NOT NULL,
                similarity_type TEXT,
                created_at TEXT
            )
        ''')
        
        # å‰µå»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_vectors_file ON code_vectors(file_path)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_semantic_term ON semantic_index(term)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_similarity_source ON code_similarity(source_file)')
        
        conn.commit()
        conn.close()
        
        logger.info("ğŸ” å‘é‡æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")
    
    def simple_text_to_vector(self, text: str, vector_size: int = 128) -> List[float]:
        """ç°¡å–®çš„æ–‡æœ¬å‘é‡åŒ– (åŸºæ–¼å­—ç¬¦é »ç‡å’Œ n-gram)"""
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # å‰µå»ºåŸºç¤å‘é‡
        vector = [0.0] * vector_size
        
        # åŸºæ–¼å–®è©å“ˆå¸Œçš„å‘é‡åŒ–
        for i, word in enumerate(words[:vector_size//2]):
            hash_val = hash(word) % vector_size
            vector[hash_val] += 1.0
        
        # åŸºæ–¼ 2-gram çš„å‘é‡åŒ–
        for i in range(len(words) - 1):
            bigram = f"{words[i]}_{words[i+1]}"
            hash_val = hash(bigram) % vector_size
            vector[hash_val] += 0.5
        
        # åŸºæ–¼ä»£ç¢¼ç‰¹å¾µçš„å‘é‡åŒ–
        code_features = {
            'function': text.count('function'),
            'class': text.count('class'),
            'import': text.count('import'),
            'export': text.count('export'),
            'const': text.count('const'),
            'let': text.count('let'),
            'var': text.count('var'),
            'if': text.count('if'),
            'for': text.count('for'),
            'while': text.count('while')
        }
        
        # å°‡ä»£ç¢¼ç‰¹å¾µåŠ å…¥å‘é‡
        for i, (feature, count) in enumerate(code_features.items()):
            if i < vector_size//4:
                vector[-(i+1)] = count
        
        # æ­£è¦åŒ–å‘é‡
        magnitude = sum(x*x for x in vector) ** 0.5
        if magnitude > 0:
            vector = [x / magnitude for x in vector]
        
        return vector
    
    def add_code_vector(self, file_path: str, content: str, metadata: Dict[str, Any] = None):
        """æ·»åŠ ä»£ç¢¼å‘é‡"""
        
        if metadata is None:
            metadata = {}
        
        # ç”Ÿæˆå‘é‡
        vector = self.simple_text_to_vector(content)
        content_hash = hashlib.md5(content.encode()).hexdigest()
        vector_id = hashlib.md5(f"{file_path}{content_hash}".encode()).hexdigest()
        
        # å­˜å„²åˆ°æ•¸æ“šåº«
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        cursor.execute('''
            INSERT OR REPLACE INTO code_vectors 
            (id, file_path, content_hash, content_text, vector_data, metadata, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            vector_id, file_path, content_hash, content[:1000],  # åªå­˜å‰1000å­—ç¬¦
            json.dumps(vector), json.dumps(metadata), now, now
        ))
        
        conn.commit()
        conn.close()
        
        # ç·©å­˜å‘é‡
        self.vector_cache[vector_id] = vector
        
        # å»ºç«‹èªç¾©ç´¢å¼•
        self.build_semantic_index(file_path, content)
        
        logger.info(f"ğŸ“Š æ·»åŠ ä»£ç¢¼å‘é‡: {file_path}")
        return vector_id
    
    def build_semantic_index(self, file_path: str, content: str):
        """å»ºç«‹èªç¾©ç´¢å¼•"""
        
        # æå–é—œéµè©
        keywords = self.extract_keywords(content)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        now = datetime.now().isoformat()
        
        # æ¸…é™¤èˆŠç´¢å¼•
        cursor.execute('DELETE FROM semantic_index WHERE file_path = ?', (file_path,))
        
        # æ·»åŠ æ–°ç´¢å¼•
        for keyword, score in keywords.items():
            cursor.execute('''
                INSERT INTO semantic_index (term, file_path, relevance_score, context, created_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (keyword, file_path, score, self.get_keyword_context(content, keyword), now))
        
        conn.commit()
        conn.close()
    
    def extract_keywords(self, content: str) -> Dict[str, float]:
        """æå–é—œéµè©å’Œç›¸é—œæ€§åˆ†æ•¸"""
        
        keywords = {}
        
        # ç·¨ç¨‹é—œéµè©
        programming_keywords = [
            'function', 'class', 'interface', 'type', 'const', 'let', 'var',
            'import', 'export', 'default', 'async', 'await', 'promise',
            'react', 'component', 'hook', 'state', 'props', 'jsx', 'tsx',
            'typescript', 'javascript', 'node', 'express', 'api', 'database',
            'test', 'jest', 'playwright', 'cypress', 'mock', 'spec'
        ]
        
        content_lower = content.lower()
        
        for keyword in programming_keywords:
            count = content_lower.count(keyword)
            if count > 0:
                # è¨ˆç®—ç›¸é—œæ€§åˆ†æ•¸ (åŸºæ–¼é »ç‡å’Œä½ç½®)
                score = min(count / 10.0, 1.0)  # æœ€å¤§åˆ†æ•¸ç‚º 1.0
                keywords[keyword] = score
        
        # æå–è‡ªå®šç¾©æ¨™è­˜ç¬¦
        identifiers = re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', content)
        identifier_counts = {}
        
        for identifier in identifiers:
            if len(identifier) > 2 and identifier not in programming_keywords:
                identifier_counts[identifier] = identifier_counts.get(identifier, 0) + 1
        
        # æ·»åŠ é«˜é »æ¨™è­˜ç¬¦
        for identifier, count in identifier_counts.items():
            if count >= 3:  # å‡ºç¾3æ¬¡ä»¥ä¸Šçš„æ¨™è­˜ç¬¦
                score = min(count / 20.0, 0.8)  # è‡ªå®šç¾©æ¨™è­˜ç¬¦æœ€å¤§åˆ†æ•¸ç‚º 0.8
                keywords[identifier] = score
        
        return keywords
    
    def get_keyword_context(self, content: str, keyword: str) -> str:
        """ç²å–é—œéµè©ä¸Šä¸‹æ–‡"""
        
        lines = content.split('\n')
        context_lines = []
        
        for line in lines:
            if keyword.lower() in line.lower():
                context_lines.append(line.strip())
                if len(context_lines) >= 3:  # æœ€å¤š3è¡Œä¸Šä¸‹æ–‡
                    break
        
        return ' | '.join(context_lines)
    
    def semantic_search(self, query: str, limit: int = 20) -> List[Dict[str, Any]]:
        """èªç¾©æœç´¢"""
        
        query_lower = query.lower()
        query_keywords = query_lower.split()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æœç´¢åŒ¹é…çš„é—œéµè©
        results = {}
        
        for keyword in query_keywords:
            cursor.execute('''
                SELECT file_path, relevance_score, context
                FROM semantic_index 
                WHERE term LIKE ? 
                ORDER BY relevance_score DESC
            ''', (f'%{keyword}%',))
            
            rows = cursor.fetchall()
            
            for file_path, score, context in rows:
                if file_path not in results:
                    results[file_path] = {
                        'file_path': file_path,
                        'total_score': 0.0,
                        'matched_terms': [],
                        'contexts': []
                    }
                
                results[file_path]['total_score'] += score
                results[file_path]['matched_terms'].append(keyword)
                results[file_path]['contexts'].append(context)
        
        conn.close()
        
        # æ’åºçµæœ
        sorted_results = sorted(results.values(), key=lambda x: x['total_score'], reverse=True)
        
        return sorted_results[:limit]
    
    def find_similar_code(self, file_path: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ç›¸ä¼¼çš„ä»£ç¢¼"""
        
        # ç²å–ç›®æ¨™æª”æ¡ˆçš„å‘é‡
        target_vector = self.get_file_vector(file_path)
        if not target_vector:
            return []
        
        # ç²å–æ‰€æœ‰å…¶ä»–å‘é‡
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT file_path, vector_data FROM code_vectors 
            WHERE file_path != ?
        ''', (file_path,))
        
        rows = cursor.fetchall()
        conn.close()
        
        similarities = []
        
        for other_file, vector_data in rows:
            try:
                other_vector = json.loads(vector_data)
                similarity = self.cosine_similarity(target_vector, other_vector)
                
                if similarity > 0.1:  # åªè¿”å›ç›¸ä¼¼åº¦ > 0.1 çš„çµæœ
                    similarities.append({
                        'file_path': other_file,
                        'similarity': similarity
                    })
            except:
                continue
        
        # æ’åºä¸¦è¿”å›
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        return similarities[:limit]
    
    def get_file_vector(self, file_path: str) -> Optional[List[float]]:
        """ç²å–æª”æ¡ˆå‘é‡"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT vector_data FROM code_vectors WHERE file_path = ?', (file_path,))
        row = cursor.fetchone()
        conn.close()
        
        if row:
            try:
                return json.loads(row[0])
            except:
                pass
        
        return None
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def get_statistics(self) -> Dict[str, Any]:
        """ç²å–çµ±è¨ˆä¿¡æ¯"""
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM code_vectors')
        total_vectors = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(*) FROM semantic_index')
        total_terms = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT term) FROM semantic_index')
        unique_terms = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'total_vectors': total_vectors,
            'total_indexed_terms': total_terms,
            'unique_terms': unique_terms,
            'cache_size': len(self.vector_cache),
            'database_path': self.db_path
        }

class AugmentVectorEnhancer:
    """Augment å‘é‡å¢å¼·å™¨"""
    
    def __init__(self):
        self.vector_db = SimpleVectorDatabase()
        self.indexed_files = set()
        
    def index_project_files(self, project_root: str = "."):
        """ç´¢å¼•é …ç›®æª”æ¡ˆ"""
        
        project_path = Path(project_root)
        
        # è¦ç´¢å¼•çš„æª”æ¡ˆé¡å‹
        file_patterns = ["**/*.py", "**/*.js", "**/*.ts", "**/*.tsx", "**/*.jsx"]
        
        indexed_count = 0
        
        for pattern in file_patterns:
            for file_path in project_path.glob(pattern):
                if self.should_index_file(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # æ·»åŠ åˆ°å‘é‡æ•¸æ“šåº«
                        self.vector_db.add_code_vector(
                            str(file_path),
                            content,
                            {
                                'language': self.detect_language(file_path),
                                'size': len(content),
                                'lines': len(content.split('\n'))
                            }
                        )
                        
                        self.indexed_files.add(str(file_path))
                        indexed_count += 1
                        
                        if indexed_count % 10 == 0:
                            logger.info(f"å·²ç´¢å¼• {indexed_count} å€‹æª”æ¡ˆ...")
                            
                    except Exception as e:
                        logger.warning(f"ç´¢å¼•æª”æ¡ˆå¤±æ•— {file_path}: {e}")
        
        logger.info(f"âœ… é …ç›®ç´¢å¼•å®Œæˆï¼Œå…±ç´¢å¼• {indexed_count} å€‹æª”æ¡ˆ")
        return indexed_count
    
    def should_index_file(self, file_path: Path) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ç´¢å¼•æª”æ¡ˆ"""
        
        # æ’é™¤ç›®éŒ„
        exclude_dirs = ['node_modules', '.git', 'dist', 'build', '.next', 'coverage']
        
        for exclude_dir in exclude_dirs:
            if exclude_dir in str(file_path):
                return False
        
        # æª”æ¡ˆå¤§å°é™åˆ¶ (æœ€å¤§ 1MB)
        try:
            if file_path.stat().st_size > 1024 * 1024:
                return False
        except:
            return False
        
        return True
    
    def detect_language(self, file_path: Path) -> str:
        """æª¢æ¸¬æª”æ¡ˆèªè¨€"""
        suffix = file_path.suffix.lower()
        
        language_map = {
            '.py': 'python',
            '.js': 'javascript',
            '.ts': 'typescript',
            '.tsx': 'typescript',
            '.jsx': 'javascript'
        }
        
        return language_map.get(suffix, 'unknown')
    
    def smart_code_search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """æ™ºèƒ½ä»£ç¢¼æœç´¢"""
        
        results = self.vector_db.semantic_search(query, limit)
        
        # å¢å¼·çµæœä¿¡æ¯
        enhanced_results = []
        
        for result in results:
            enhanced_result = result.copy()
            enhanced_result['search_query'] = query
            enhanced_result['relevance_percentage'] = min(100, int(result['total_score'] * 100))
            enhanced_results.append(enhanced_result)
        
        return enhanced_results
    
    def find_code_patterns(self, file_path: str) -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ä»£ç¢¼æ¨¡å¼"""
        
        similar_files = self.vector_db.find_similar_code(file_path)
        
        patterns = []
        for similar in similar_files:
            if similar['similarity'] > 0.5:  # é«˜ç›¸ä¼¼åº¦
                patterns.append({
                    'pattern_type': 'high_similarity',
                    'related_file': similar['file_path'],
                    'similarity_score': similar['similarity'],
                    'description': f"èˆ‡ {similar['file_path']} æœ‰ {similar['similarity']:.2%} çš„ç›¸ä¼¼åº¦"
                })
        
        return patterns
    
    def get_enhancement_statistics(self) -> Dict[str, Any]:
        """ç²å–å¢å¼·çµ±è¨ˆä¿¡æ¯"""
        
        vector_stats = self.vector_db.get_statistics()
        
        return {
            'indexed_files': len(self.indexed_files),
            'vector_database': vector_stats,
            'search_capabilities': [
                'èªç¾©ä»£ç¢¼æœç´¢',
                'ç›¸ä¼¼ä»£ç¢¼æª¢æ¸¬',
                'æ¨¡å¼è­˜åˆ¥',
                'æ™ºèƒ½å»ºè­°'
            ]
        }

def main():
    """æ¸¬è©¦å‘é‡å¢å¼·å™¨"""
    
    print("ğŸ” åˆå§‹åŒ– Augment å‘é‡å¢å¼·å™¨...")
    
    enhancer = AugmentVectorEnhancer()
    
    # ç´¢å¼•ç•¶å‰é …ç›®
    print("ğŸ“Š é–‹å§‹ç´¢å¼•é …ç›®æª”æ¡ˆ...")
    indexed_count = enhancer.index_project_files()
    
    # æ¸¬è©¦æœç´¢
    print("\nğŸ” æ¸¬è©¦èªç¾©æœç´¢...")
    search_results = enhancer.smart_code_search("memory system database")
    
    print(f"æ‰¾åˆ° {len(search_results)} å€‹ç›¸é—œçµæœ:")
    for result in search_results[:3]:
        print(f"  ğŸ“„ {result['file_path']} (ç›¸é—œæ€§: {result['relevance_percentage']}%)")
    
    # ç²å–çµ±è¨ˆä¿¡æ¯
    stats = enhancer.get_enhancement_statistics()
    print(f"\nğŸ“Š å¢å¼·çµ±è¨ˆ:")
    print(f"  ç´¢å¼•æª”æ¡ˆ: {stats['indexed_files']}")
    print(f"  å‘é‡æ•¸é‡: {stats['vector_database']['total_vectors']}")
    print(f"  ç´¢å¼•è©å½™: {stats['vector_database']['unique_terms']}")
    
    print("âœ… å‘é‡å¢å¼·å™¨æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main()
