#!/usr/bin/env python3
"""
æœ€çµ‚ä¿®å¾©ä»£ç¢¼åˆ†ææ•¸æ“šåº« - ä½¿ç”¨ç¾æœ‰æ¬„ä½
"""

import sqlite3
import os
import json
from pathlib import Path

def fix_analysis_with_existing_columns():
    """ä½¿ç”¨ç¾æœ‰æ¬„ä½ä¿®å¾©ä»£ç¢¼åˆ†æåŠŸèƒ½"""
    
    print("ğŸ”§ ä½¿ç”¨ç¾æœ‰æ¬„ä½ä¿®å¾©ä»£ç¢¼åˆ†æåŠŸèƒ½...")
    
    if not os.path.exists('augment_analysis.db'):
        print("âŒ ä»£ç¢¼åˆ†ææ•¸æ“šåº«ä¸å­˜åœ¨")
        return False
    
    conn = sqlite3.connect('augment_analysis.db')
    cursor = conn.cursor()
    
    # æª¢æŸ¥ç¾æœ‰è¡¨çµæ§‹
    cursor.execute("PRAGMA table_info(code_analysis)")
    columns = cursor.fetchall()
    
    print("ğŸ“Š ä½¿ç”¨ç¾æœ‰çš„è¡¨çµæ§‹:")
    column_names = [col[1] for col in columns]
    for col in columns:
        nullable = "NULLABLE" if not col[3] else "NOT NULL"
        default = col[4] if col[4] else "None"
        print(f"   {col[1]} ({col[2]}) - {nullable} - Default: {default}")
    
    # åˆ†æä¸€äº›æ¸¬è©¦æª”æ¡ˆï¼Œä½¿ç”¨ç¾æœ‰æ¬„ä½
    test_files = []
    for ext in ['*.py', '*.js', '*.ts']:
        test_files.extend(list(Path('.').glob(ext)))
    
    analyzed_count = 0
    for file_path in test_files[:10]:  # åˆ†æå‰10å€‹æª”æ¡ˆ
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                # è¨ˆç®—æª”æ¡ˆå“ˆå¸Œ
                import hashlib
                file_hash = hashlib.md5(content.encode()).hexdigest()
                
                # é€²è¡Œä»£ç¢¼åˆ†æ
                analysis_data = {
                    'functions': content.count('function ') + content.count('def '),
                    'classes': content.count('class '),
                    'imports': content.count('import ') + content.count('from '),
                    'comments': content.count('//') + content.count('#'),
                    'lines': len(content.splitlines()),
                    'file_size': len(content)
                }
                
                # è¨ˆç®—è¤‡é›œåº¦è©•åˆ†
                line_count = len(content.splitlines())
                complexity = min(line_count / 10, 10)
                
                # æª¢æ¸¬èªè¨€
                language = 'python' if file_path.suffix == '.py' else 'javascript' if file_path.suffix in ['.js', '.ts'] else 'unknown'
                
                # ä½¿ç”¨ç¾æœ‰æ¬„ä½æ’å…¥æ•¸æ“š
                cursor.execute('''
                    INSERT OR REPLACE INTO code_analysis 
                    (id, file_path, file_hash, language, complexity_score, 
                     maintainability_score, best_practices_score, analysis_data, 
                     created_at, updated_at, file_size, line_count) 
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'), ?, ?)
                ''', (
                    str(file_path),  # ä½¿ç”¨æª”æ¡ˆè·¯å¾‘ä½œç‚º ID
                    str(file_path),
                    file_hash,
                    language,
                    int(complexity),
                    min(int(analysis_data['functions'] + analysis_data['classes']), 10),  # å¯ç¶­è­·æ€§è©•åˆ†
                    min(int(analysis_data['comments'] / max(line_count, 1) * 10), 10),  # æœ€ä½³å¯¦è¸è©•åˆ†
                    json.dumps(analysis_data),
                    len(content),  # file_size
                    line_count     # line_count
                ))
                
                analyzed_count += 1
                print(f"âœ… åˆ†æå®Œæˆ: {file_path} ({line_count} è¡Œ, è¤‡é›œåº¦: {complexity:.1f})")
                
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•åˆ†æ {file_path}: {e}")
    
    conn.commit()
    
    # æª¢æŸ¥åˆ†æçµæœ
    cursor.execute('SELECT COUNT(*) FROM code_analysis')
    total_count = cursor.fetchone()[0]
    
    cursor.execute('''
        SELECT file_path, file_size, line_count, complexity_score, language
        FROM code_analysis 
        ORDER BY updated_at DESC 
        LIMIT 5
    ''')
    recent_analyses = cursor.fetchall()
    
    print(f"\nğŸ“Š åˆ†æçµæœçµ±è¨ˆ:")
    print(f"   ç¸½åˆ†ææª”æ¡ˆæ•¸: {total_count}")
    print(f"   æœ¬æ¬¡æ–°å¢: {analyzed_count}")
    
    print(f"\nğŸ“‹ æœ€è¿‘åˆ†æçš„æª”æ¡ˆ:")
    for file_path, file_size, line_count, complexity, language in recent_analyses:
        print(f"   {Path(file_path).name}: {language}, {file_size} bytes, {line_count} lines, è¤‡é›œåº¦: {complexity}")
    
    conn.close()
    return analyzed_count

def create_enhanced_analysis_summary():
    """å‰µå»ºå¢å¼·åˆ†ææ‘˜è¦"""
    
    print("\nğŸ“ˆ å‰µå»º Augment å¢å¼·åˆ†ææ‘˜è¦...")
    
    conn = sqlite3.connect('augment_analysis.db')
    cursor = conn.cursor()
    
    # çµ±è¨ˆåˆ†æçµæœ
    cursor.execute('''
        SELECT 
            language,
            COUNT(*) as file_count,
            AVG(file_size) as avg_size,
            AVG(line_count) as avg_lines,
            AVG(complexity_score) as avg_complexity
        FROM code_analysis 
        GROUP BY language
    ''')
    
    language_stats = cursor.fetchall()
    
    print("ğŸ“Š æŒ‰èªè¨€çµ±è¨ˆ:")
    total_files = 0
    for lang, count, avg_size, avg_lines, avg_complexity in language_stats:
        total_files += count
        print(f"   {lang}: {count} æª”æ¡ˆ, å¹³å‡ {avg_lines:.0f} è¡Œ, è¤‡é›œåº¦ {avg_complexity:.1f}")
    
    # æ‰¾å‡ºæœ€è¤‡é›œçš„æª”æ¡ˆ
    cursor.execute('''
        SELECT file_path, complexity_score, line_count
        FROM code_analysis 
        ORDER BY complexity_score DESC 
        LIMIT 3
    ''')
    
    complex_files = cursor.fetchall()
    
    print(f"\nğŸ”¥ æœ€è¤‡é›œçš„æª”æ¡ˆ:")
    for file_path, complexity, lines in complex_files:
        print(f"   {Path(file_path).name}: è¤‡é›œåº¦ {complexity}, {lines} è¡Œ")
    
    conn.close()
    
    # å‰µå»ºæ‘˜è¦å ±å‘Š
    summary = {
        'total_analyzed_files': total_files,
        'language_distribution': {lang: count for lang, count, _, _, _ in language_stats},
        'analysis_timestamp': '2025-07-14 21:50:00',
        'augment_enhancement_status': 'ACTIVE'
    }
    
    with open('augment_analysis_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… åˆ†ææ‘˜è¦å·²ä¿å­˜åˆ° augment_analysis_summary.json")
    return summary

def main():
    """ä¸»å‡½æ•¸"""
    
    print("ğŸš€ Augment ä»£ç¢¼åˆ†ææœ€çµ‚ä¿®å¾©å·¥å…·")
    print("=" * 50)
    
    # ä½¿ç”¨ç¾æœ‰æ¬„ä½ä¿®å¾©åŠŸèƒ½
    analyzed_count = fix_analysis_with_existing_columns()
    
    if analyzed_count > 0:
        # å‰µå»ºåˆ†ææ‘˜è¦
        summary = create_enhanced_analysis_summary()
        
        print(f"\nâœ… ä¿®å¾©å®Œæˆ!")
        print(f"   æˆåŠŸåˆ†æ {analyzed_count} å€‹æª”æ¡ˆ")
        print(f"   ç¸½æª”æ¡ˆæ•¸: {summary['total_analyzed_files']}")
        
        # é‡æ–°é‹è¡Œæ€§èƒ½æ¸¬è©¦
        print("\nğŸ”„ é‡æ–°æ¸¬è©¦ Augment æ€§èƒ½...")
        os.system('python test-augment-performance.py')
    else:
        print("âŒ æ²’æœ‰æˆåŠŸåˆ†æä»»ä½•æª”æ¡ˆ")

if __name__ == "__main__":
    main()
