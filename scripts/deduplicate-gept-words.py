#!/usr/bin/env python3
"""
å»é‡ä¸¦åˆ†ç´š GEPT è©å½™
å¾é«˜ç´šåˆ¥ä¸­ç§»é™¤ä½ç´šåˆ¥çš„å–®å­—,ç²å¾—æ¯å€‹ç´šåˆ¥çš„ç¨ç‰¹å–®å­—
"""

import sys

def load_words(file_path):
    """å¾æ–‡ä»¶åŠ è¼‰å–®å­—"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            words = set(line.strip().lower() for line in f if line.strip())
        return words
    except Exception as e:
        print(f"âŒ è®€å–æ–‡ä»¶å¤±æ•— {file_path}: {e}")
        return set()

def save_words(words, file_path):
    """ä¿å­˜å–®å­—åˆ°æ–‡ä»¶"""
    try:
        sorted_words = sorted(words)
        with open(file_path, 'w', encoding='utf-8') as f:
            for word in sorted_words:
                f.write(f"{word}\n")
        print(f"âœ… ä¿å­˜ {len(sorted_words)} å€‹å–®å­—åˆ°: {file_path}")
        return len(sorted_words)
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±æ•— {file_path}: {e}")
        return 0

def deduplicate_gept_words():
    """å»é‡ä¸¦åˆ†ç´š GEPT è©å½™"""
    
    print("=== GEPT è©å½™å»é‡èˆ‡åˆ†ç´š ===\n")
    
    # 1. åŠ è¼‰æ‰€æœ‰ç´šåˆ¥çš„å–®å­—
    print("ğŸ“– æ­£åœ¨åŠ è¼‰å–®å­—...")
    elementary = load_words('data/word-lists/gept-elementary-pdf.txt')
    intermediate = load_words('data/word-lists/gept-intermediate-pdf.txt')
    high_intermediate = load_words('data/word-lists/gept-high-intermediate-pdf.txt')
    
    print(f"åˆç´š (åŸå§‹): {len(elementary)} å€‹å–®å­—")
    print(f"ä¸­ç´š (åŸå§‹): {len(intermediate)} å€‹å–®å­—")
    print(f"ä¸­é«˜ç´š (åŸå§‹): {len(high_intermediate)} å€‹å–®å­—")
    print()
    
    # 2. å»é‡ - å¾é«˜ç´šåˆ¥ä¸­ç§»é™¤ä½ç´šåˆ¥çš„å–®å­—
    print("ğŸ”„ æ­£åœ¨å»é‡...")
    
    # ä¸­ç´š = ä¸­ç´š - åˆç´š
    intermediate_unique = intermediate - elementary
    
    # ä¸­é«˜ç´š = ä¸­é«˜ç´š - ä¸­ç´š - åˆç´š
    high_intermediate_unique = high_intermediate - intermediate - elementary
    
    print(f"åˆç´š (å»é‡å¾Œ): {len(elementary)} å€‹å–®å­— (ç„¡è®ŠåŒ–)")
    print(f"ä¸­ç´š (å»é‡å¾Œ): {len(intermediate_unique)} å€‹å–®å­— (ç§»é™¤äº† {len(intermediate) - len(intermediate_unique)} å€‹)")
    print(f"ä¸­é«˜ç´š (å»é‡å¾Œ): {len(high_intermediate_unique)} å€‹å–®å­— (ç§»é™¤äº† {len(high_intermediate) - len(high_intermediate_unique)} å€‹)")
    print()
    
    # 3. ä¿å­˜å»é‡å¾Œçš„å–®å­—
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜å»é‡å¾Œçš„å–®å­—...")
    
    save_words(elementary, 'data/word-lists/gept-elementary-unique.txt')
    save_words(intermediate_unique, 'data/word-lists/gept-intermediate-unique.txt')
    save_words(high_intermediate_unique, 'data/word-lists/gept-high-intermediate-unique.txt')
    
    print()
    
    # 4. çµ±è¨ˆç¸½çµ
    total_unique = len(elementary) + len(intermediate_unique) + len(high_intermediate_unique)
    total_original = len(elementary) + len(intermediate) + len(high_intermediate)
    
    print("=" * 60)
    print("ğŸ“Š çµ±è¨ˆç¸½çµ")
    print("=" * 60)
    print(f"åŸå§‹ç¸½å–®å­—æ•¸: {total_original}")
    print(f"å»é‡å¾Œç¸½å–®å­—æ•¸: {total_unique}")
    print(f"ç§»é™¤é‡è¤‡: {total_original - total_unique} å€‹")
    print()
    print("å„ç´šåˆ¥ç¨ç‰¹å–®å­—:")
    print(f"  - åˆç´š: {len(elementary)} å€‹")
    print(f"  - ä¸­ç´š: {len(intermediate_unique)} å€‹")
    print(f"  - ä¸­é«˜ç´š: {len(high_intermediate_unique)} å€‹")
    print("=" * 60)
    
    # 5. é¡¯ç¤ºä¸€äº›ç¯„ä¾‹
    print("\nğŸ“ ç¯„ä¾‹å–®å­—:")
    print("\nåˆç´š (å‰ 10 å€‹):")
    for word in sorted(elementary)[:10]:
        print(f"  - {word}")
    
    print("\nä¸­ç´š (å‰ 10 å€‹):")
    for word in sorted(intermediate_unique)[:10]:
        print(f"  - {word}")
    
    print("\nä¸­é«˜ç´š (å‰ 10 å€‹):")
    for word in sorted(high_intermediate_unique)[:10]:
        print(f"  - {word}")

if __name__ == "__main__":
    deduplicate_gept_words()

